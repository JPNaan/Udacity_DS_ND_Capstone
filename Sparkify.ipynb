{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparkify Project Workspace\n",
    "This workspace contains a tiny subset (128MB) of the full dataset available (12GB). Feel free to use this workspace to build your project, or to explore a smaller subset with Spark before deploying your cluster on the cloud. Instructions for setting up your Spark cluster is included in the last lesson of the Extracurricular Spark Course content.\n",
    "\n",
    "You can follow the steps below to guide your data analysis and model building portion of this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator, RegressionEvaluator, BinaryClassificationEvaluator\n",
    "from pyspark.ml.feature import StandardScaler, VectorAssembler\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a Spark session\n",
    "spark = SparkSession.builder.appName(\"Sparkify\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset\n",
    "In this workspace, the mini-dataset file is `mini_sparkify_event_data.json`. \n",
    "Additional datasets can be found at:\n",
    " - s3n://udacity-dsnd/sparkify/mini_sparkify_event_data.json (larger dataset located in AWS s3)\n",
    " - s3n://udacity-dsnd/sparkify/sparkify_event_data.json  (full dataset located in AWS s3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load in data\n",
    "event_data = 'mini_sparkify_event_data.json'\n",
    "df = spark.read.json(event_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis\n",
    "Here we will be exploring the data schema & values.  In particular we will be looking for qualitative characteristics from the data, and how it relates to particular users.  We will also be looking in detal for data anomolies such as missing or incomplete values (for example null userIds or sessionIds)\n",
    "\n",
    "***KEY OUPUT***\n",
    "A key output of the analysis will be how to construct a lable column identifying if a member has 'churned'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- artist: string (nullable = true)\n",
      " |-- auth: string (nullable = true)\n",
      " |-- firstName: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- itemInSession: long (nullable = true)\n",
      " |-- lastName: string (nullable = true)\n",
      " |-- length: double (nullable = true)\n",
      " |-- level: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- method: string (nullable = true)\n",
      " |-- page: string (nullable = true)\n",
      " |-- registration: long (nullable = true)\n",
      " |-- sessionId: long (nullable = true)\n",
      " |-- song: string (nullable = true)\n",
      " |-- status: long (nullable = true)\n",
      " |-- ts: long (nullable = true)\n",
      " |-- userAgent: string (nullable = true)\n",
      " |-- userId: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Review file schema\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|level|\n",
      "+-----+\n",
      "| free|\n",
      "| paid|\n",
      "+-----+\n",
      "\n",
      "+------+\n",
      "|method|\n",
      "+------+\n",
      "|   PUT|\n",
      "|   GET|\n",
      "+------+\n",
      "\n",
      "+--------------------+\n",
      "|                page|\n",
      "+--------------------+\n",
      "|              Cancel|\n",
      "|    Submit Downgrade|\n",
      "|         Thumbs Down|\n",
      "|                Home|\n",
      "|           Downgrade|\n",
      "|         Roll Advert|\n",
      "|              Logout|\n",
      "|       Save Settings|\n",
      "|Cancellation Conf...|\n",
      "|               About|\n",
      "| Submit Registration|\n",
      "|            Settings|\n",
      "|               Login|\n",
      "|            Register|\n",
      "|     Add to Playlist|\n",
      "|          Add Friend|\n",
      "|            NextSong|\n",
      "|           Thumbs Up|\n",
      "|                Help|\n",
      "|             Upgrade|\n",
      "|               Error|\n",
      "|      Submit Upgrade|\n",
      "+--------------------+\n",
      "\n",
      "+------+\n",
      "|status|\n",
      "+------+\n",
      "|   307|\n",
      "|   404|\n",
      "|   200|\n",
      "+------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None, None, None)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# understand unique qualitative values\n",
    "df.select(['level']).dropDuplicates().show(),\\\n",
    "    df.select(['method']).dropDuplicates().show(),\\\n",
    "    df.select(['page']).dropDuplicates().show(50),\\\n",
    "    df.select(['status']).dropDuplicates().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------+\n",
      "|length(userId)| count|\n",
      "+--------------+------+\n",
      "|             0|  8346|\n",
      "|             1| 11300|\n",
      "|             2|128858|\n",
      "|             3| 67340|\n",
      "|             6| 70656|\n",
      "+--------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Understand userID format\n",
    "df.groupBy(F.length(F.col('userId'))).count().orderBy('length(userId)').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----+\n",
      "|length(userId)|count|\n",
      "+--------------+-----+\n",
      "|             0|    1|\n",
      "|             1|    8|\n",
      "|             2|   88|\n",
      "|             3|   56|\n",
      "|             6|   73|\n",
      "+--------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Understand unique userId format\n",
    "df.select(df.userId).dropDuplicates().groupBy(F.length(F.col('userId'))).count().orderBy('length(userId)').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----+\n",
      "|length(sessionId)|count|\n",
      "+-----------------+-----+\n",
      "|                1|    9|\n",
      "|                2|   88|\n",
      "|                3|  897|\n",
      "|                4| 1360|\n",
      "+-----------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#understand unique sessionID format\n",
    "df.select(df.sessionId).dropDuplicates().groupBy(F.length(F.col('sessionId'))).count().orderBy('length(sessionId)').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tools to further investigate the data by particular characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-------------+\n",
      "|           page|itemInSession|\n",
      "+---------------+-------------+\n",
      "|       NextSong|           50|\n",
      "|       NextSong|           51|\n",
      "|       NextSong|           52|\n",
      "|       NextSong|           53|\n",
      "|Add to Playlist|           54|\n",
      "|       NextSong|           55|\n",
      "|       NextSong|           56|\n",
      "|       NextSong|           57|\n",
      "|       NextSong|           58|\n",
      "|       NextSong|           59|\n",
      "|       NextSong|           60|\n",
      "|       NextSong|           61|\n",
      "|       NextSong|           62|\n",
      "|       NextSong|           63|\n",
      "|       NextSong|           64|\n",
      "|       NextSong|           65|\n",
      "|       NextSong|           66|\n",
      "|       NextSong|           67|\n",
      "|       NextSong|           68|\n",
      "|       NextSong|           69|\n",
      "+---------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Understand itemsInSession\n",
    "df.select(['page', 'itemInSession']).where(\"userId == 30 and sessionId == 29\").orderBy('itemInSession').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+\n",
      "|sessionId|count|\n",
      "+---------+-----+\n",
      "|       29|   42|\n",
      "|      264|    2|\n",
      "|      532|  290|\n",
      "|      614|    6|\n",
      "|      682|    2|\n",
      "|      757|   27|\n",
      "|      763|   32|\n",
      "|      816|   77|\n",
      "|      892|    2|\n",
      "|      945|    1|\n",
      "|     1110|   31|\n",
      "|     1184|   33|\n",
      "|     1304|    3|\n",
      "|     1311|   60|\n",
      "|     1340|  106|\n",
      "|     1477|   79|\n",
      "|     1496|   31|\n",
      "|     1554|   97|\n",
      "|     1574|   35|\n",
      "|     1607|   28|\n",
      "+---------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.where(df.userId == 30).groupBy('sessionId').count().orderBy('sessionId').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+----------+-------------+\n",
      "|userId|sessionId|      page|itemInSession|\n",
      "+------+---------+----------+-------------+\n",
      "|    30|      264|  NextSong|            0|\n",
      "|    30|      264|Add Friend|            1|\n",
      "+------+---------+----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(['userId', 'sessionId', 'page', 'itemInSession'])\\\n",
    "    .where((df.userId == '30') & (df.sessionId == '264'))\\\n",
    "    .orderBy('itemInSession')\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+----+-------------+\n",
      "|userId|sessionId|page|itemInSession|\n",
      "+------+---------+----+-------------+\n",
      "+------+---------+----+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(['userId', 'sessionId', 'page', 'itemInSession'])\\\n",
    "    .where((df.userId == '100010') & (df.page == 'Cancellation Confirmation'))\\\n",
    "    .orderBy('itemInSession')\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution of engagement data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "thumbs_df = df.select('userId', 'page').where((df.page == 'Thumbs Up') | (df.page == 'Thumbs.down')).groupBy('userId').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution of thumbs up or down activity\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ff284e0e550>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('Distribution of thumbs up or down activity')\n",
    "plt.hist(thumbs_df.select('count').rdd.flatMap(lambda x: x).collect());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sessions_df = df.select('userId', 'sessionId', 'itemInSession').groupBy('userId','sessionId').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution of items in users sessions\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ff270864eb8>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('Distribution of items in users sessions')\n",
    "plt.hist(sessions_df.select('count').rdd.flatMap(lambda x: x).collect());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review Null & Blank data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total count:  286500\n",
      "count with dropped null sessionId or userId:  286500\n"
     ]
    }
   ],
   "source": [
    "print('total count: ', df.count())\n",
    "print('count with dropped null sessionId or userId: ', df.dropna(how = \"any\", subset = [\"userId\", \"sessionId\"]).count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Data\n",
    "We have identified that although there are no null values for the userId, we do have a number of values with the userId blank.  However, since we want to be able to leverage this code on additional data sets we will go ahead and handle the case for null data as well as this case for blank strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows where either userId or sessionId are blank\n",
    "df = df.where('''\n",
    "                length(userId) > 0\n",
    "                AND\n",
    "                length(sessionId) > 0\n",
    "                ''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop any rows with either sessionId or userId null\n",
    "df = df.dropna(how = \"any\", subset = [\"userId\", \"sessionId\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of Records:  278154\n",
      "# of Users:  225\n"
     ]
    }
   ],
   "source": [
    "print('# of Records: ', df.count())\n",
    "print('# of Users: ', df.select('userId').dropDuplicates().count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "Now that we are familiar with the data we will build out the following features that will be used in the modeling step.  Since we are interested in identifying the churn of individual users, our features will be constructed by a number of engagement metrics.\n",
    "\n",
    "**Label**\n",
    "- churn: represents any user who has had the 'Cancelation Confirmation' event.  This will be the target label for our modeling as we are interested in identifying users who may cancel their account\n",
    "\n",
    "**Identifier**\n",
    "- userId:  represents the unique users\n",
    "\n",
    "**Engagement Metrics**\n",
    "- no_thumbs:  represents the number of thumbs up or thumbs down that a user performed \n",
    "- no_playlist: represents the number of songs that a user has added to their playlists \n",
    "- no_friends: represents the number of friends added to their account\n",
    "- no_help: represents the number of times a user had to access the help feature \n",
    "- no_settings: represents the number of times a user accessed the settings menu\n",
    "- no_advert:  represents the number of times that a user received an ad\n",
    "- no_errors:  represents the number of times a user received an error message\n",
    "- avg_pages:  represents the average number of pages that the users accessed by session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a temp veiw to enable spark.sql functionality\n",
    "df.createOrReplaceTempView(\"df_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature engineering script\n",
    "engagement = spark.sql('''\n",
    "SELECT userId,\n",
    "       SUM(CASE WHEN page = 'Cancellation Confirmation' THEN 1 ELSE 0 END) AS churn,\n",
    "       SUM(CASE WHEN page = 'Thumbs Up' OR page = 'Thumbs Down' THEN 1 ELSE 0 END) AS no_thumbs,\n",
    "       SUM(CASE When page = 'Add to Playlist' THEN 1 ELSE 0 END) AS no_playlist,\n",
    "       SUM(CASE when page = 'Add Friend' THEN 1 ELSE 0 END) AS no_friends,\n",
    "       SUM(CASE when page = 'Help' THEN 1 ELSE 0 END) AS no_help,\n",
    "       SUM(CASE when page = 'Settings' THEN 1 ELSE 0 END) AS no_settings,\n",
    "       SUM(CASE when page = 'Roll Advert' THEN 1 ELSE 0 END) AS no_advert,\n",
    "       SUM(CASE when page = 'Error' THEN 1 ELSE 0 END) AS no_errors\n",
    "FROM df_table AS engagement\n",
    "GROUP BY userId\n",
    "''')\n",
    "\n",
    "sessions = spark.sql('''\n",
    "select userId, ROUND(AVG(pages)) as avg_pages\n",
    "from (select userId, sessionId, count(itemInSession) as pages from df_table group by userId, sessionId order by userId) \n",
    "group by userId''')\n",
    "\n",
    "feature_set = engagement.join(sessions, ['userId'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+---------+-----------+----------+-------+-----------+---------+---------+---------+\n",
      "|userId|churn|no_thumbs|no_playlist|no_friends|no_help|no_settings|no_advert|no_errors|avg_pages|\n",
      "+------+-----+---------+-----------+----------+-------+-----------+---------+---------+---------+\n",
      "|100010|    0|       22|          7|         4|      2|          0|       52|        0|     54.0|\n",
      "|200002|    0|       27|          8|         4|      2|          3|        7|        0|     79.0|\n",
      "|   125|    1|        0|          0|         0|      0|          0|        1|        0|     11.0|\n",
      "|   124|    0|      212|        118|        74|     23|         20|        4|        6|    166.0|\n",
      "|    51|    1|      121|         52|        28|     12|         12|        0|        1|    246.0|\n",
      "|     7|    0|        8|          5|         1|      1|          3|       16|        1|     29.0|\n",
      "|    15|    0|       95|         59|        31|      8|         16|        1|        2|    152.0|\n",
      "|    54|    1|      192|         72|        33|     17|         17|       47|        1|     93.0|\n",
      "|   155|    0|       61|         24|        11|      9|          5|        8|        3|    167.0|\n",
      "|100014|    1|       20|          7|         6|      2|          1|        2|        0|     52.0|\n",
      "|   132|    0|      113|         38|        41|     16|         17|        2|        3|    144.0|\n",
      "|   154|    0|       11|          1|         3|      1|          0|       10|        0|     39.0|\n",
      "|   101|    1|      102|         61|        29|     12|          9|        8|        3|    215.0|\n",
      "|    11|    0|       49|         20|         6|      3|          6|       39|        1|     53.0|\n",
      "|   138|    0|      119|         67|        41|     13|         17|       17|        1|    165.0|\n",
      "|300017|    0|      331|        113|        63|     27|         23|       11|        5|     70.0|\n",
      "|100021|    1|       16|          7|         7|      0|          0|       30|        2|     64.0|\n",
      "|    29|    1|      176|         89|        47|     28|         19|       22|        0|    106.0|\n",
      "|    69|    0|       81|         33|        12|      7|          3|        3|        4|    149.0|\n",
      "|   112|    0|       12|          7|         7|      1|          4|       21|        0|     29.0|\n",
      "+------+-----+---------+-----------+----------+-------+-----------+---------+---------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#The data set to model\n",
    "feature_set.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling\n",
    "We will use Random Forest and Logistic regression on our data set in order to predict the churn event.  We will setup a pipeline within cross validation to build the models.  The dataset will be split to train 80% of the data thereby leaving 20% of the data for testing.\n",
    "\n",
    "The success of our model will be determined using precision, accuracy and F1 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create train and test datasets\n",
    "train, test = feature_set.randomSplit([0.8, 0.2], seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a list containing only dataset featrues, and then load these features into a vector assembler for modeling\n",
    "feature_list = ['no_thumbs', 'no_playlist', 'no_friends', 'no_help', 'no_settings', 'no_advert', 'no_errors', 'avg_pages']\n",
    "assembler = VectorAssembler(inputCols = feature_list, outputCol = 'feature_vect')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build the random forest model pipline & parameters\n",
    "rand_forest = RandomForestRegressor(labelCol = 'churn', featuresCol = 'feature_vect')\n",
    "forest_pipeline = Pipeline(stages = [assembler, rand_forest])\n",
    "forest_paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(rand_forest.numTrees, [10]) \\\n",
    "    .addGrid(rand_forest.maxDepth, [5,10]) \\\n",
    "    .build()\n",
    "forest_crossval = CrossValidator(estimator = forest_pipeline,\n",
    "                          estimatorParamMaps = forest_paramGrid,\n",
    "                          evaluator = RegressionEvaluator(labelCol='churn'),\n",
    "                          numFolds = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fit the model with the training dataset, and then apply the model to the test dataset\n",
    "f_cvModel = forest_crossval.fit(train)\n",
    "f_predict = f_cvModel.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+-----+\n",
      "|churn|pred_round|count|\n",
      "+-----+----------+-----+\n",
      "|    0|       0.5|    1|\n",
      "|    1|       0.3|    3|\n",
      "|    0|       0.4|    1|\n",
      "|    1|       0.5|    1|\n",
      "|    1|       0.4|    1|\n",
      "|    0|       0.6|    1|\n",
      "|    0|       0.2|    8|\n",
      "|    0|       0.0|    6|\n",
      "|    0|       0.1|    4|\n",
      "|    1|       0.2|    1|\n",
      "|    0|       0.3|    5|\n",
      "|    1|       0.1|    2|\n",
      "+-----+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Take a look at the predictions to gain an uderstanding of how many churns were predicted\n",
    "f_predictions = f_predict.withColumn('pred_round',F.round(f_predict.prediction,1))\n",
    "f_predictions.groupBy('churn','pred_round').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression\n",
    "We see that the above Random Forest model failed to predict the churn events, and since we do not have any predicted positives we will get precision and F1 as zero without having to perform many calculations.  We will now take a look at the Logistic Regression model to see if we do any better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build model and parameters\n",
    "log_reg = LogisticRegression(labelCol = 'churn', featuresCol = 'feature_vect')\n",
    "reg_pipeline = Pipeline(stages = [assembler, log_reg])\n",
    "reg_paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(log_reg.maxIter, [10,20]) \\\n",
    "    .build()\n",
    "reg_crossval = CrossValidator(estimator = reg_pipeline,\n",
    "                          estimatorParamMaps = reg_paramGrid,\n",
    "                          evaluator = RegressionEvaluator(labelCol='churn'),\n",
    "                          numFolds = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit and transform the model\n",
    "reg_cvModel = reg_crossval.fit(train)\n",
    "reg_predict = reg_cvModel.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+-----+\n",
      "|churn|prediction|count|\n",
      "+-----+----------+-----+\n",
      "|    1|       0.0|    8|\n",
      "|    0|       0.0|   26|\n",
      "+-----+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Take a look at the predictions to see if we get a better result\n",
    "reg_predict.select('userId', 'churn', 'prediction').groupBy('churn', 'prediction').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*At this point we can see that neither the Random Forest nor the Logistic regression models are capable of predicting a single churn event.  As such we will scale the features using standard scaler to see if the differing value ranges within the original data set are affecting the predictability.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scale the dataset\n",
    "standardscaler = StandardScaler().setInputCol(\"feature_vect\").setOutputCol(\"scaled_features\")\n",
    "feature_std_scaled = standardscaler.fit(assembler.transform(feature_set)).transform(assembler.transform(feature_set))\n",
    "feature_std_scaled = feature_std_scaled.drop('feature_vect')                                                                                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create train and test sets\n",
    "scale_train, scale_test = feature_std_scaled.randomSplit([0.8, 0.2], seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instantiate model with pipeline and cross validation parameters\n",
    "log_reg_scale = LogisticRegression(labelCol = 'churn', featuresCol = 'scaled_features')\n",
    "reg_pipeline_scale = Pipeline(stages = [assembler, log_reg])\n",
    "reg_paramGrid_scale = ParamGridBuilder() \\\n",
    "    .addGrid(log_reg_scale.maxIter, [10]) \\\n",
    "    .build()\n",
    "reg_crossval_scale = CrossValidator(estimator = reg_pipeline_scale,\n",
    "                          estimatorParamMaps = reg_paramGrid_scale,\n",
    "                          #evaluator = RegressionEvaluator(labelCol='churn'),\n",
    "                          evaluator = BinaryClassificationEvaluator(labelCol='churn'),          \n",
    "                          numFolds = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train and test model\n",
    "reg_cvModel_scale = reg_crossval_scale.fit(scale_train)\n",
    "reg_predict_scale = reg_cvModel_scale.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+---------+-----------+----------+-------+-----------+---------+---------+---------+------------+-------------+-----------+----------+\n",
      "|userId|churn|no_thumbs|no_playlist|no_friends|no_help|no_settings|no_advert|no_errors|avg_pages|feature_vect|rawPrediction|probability|prediction|\n",
      "+------+-----+---------+-----------+----------+-------+-----------+---------+---------+---------+------------+-------------+-----------+----------+\n",
      "+------+-----+---------+-----------+----------+-------+-----------+---------+---------+---------+------------+-------------+-----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Investigate model results to see if the scaled data returned a better result\n",
    "reg_predict_scale.where(reg_predict_scale.prediction > 0).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "Here we have evaluated the Sparkify data, and built a number of engagement related features.  It is possible that running these models on the full dataset would provide us better results.  However, we were unsuccessful establishing a model that would predict the churn events despite evaluating with Random Forest and Logistic Regression on the current dataset.  As both models have similar results, and we have experimented with a few different hyper parameters, it may be best to look directly at the features for improvement.\n",
    "\n",
    "A number of possible improvements include:\n",
    "- We can look to leverage statistical analysis to replace the counts of engagement by percentile.  Taking the no_thumbs column for example, instead of just counting all the values we could take the top 10% of users and replace their value with .9, for those between 10% and 20% we could replace with .8, etc.  This would reduce some of the variability of the values since the engagement for the user with the 95% highest thumbs ranking is likely not qualitatively different than the user with 98% highest thumbs ranking.\n",
    "- We could leverage the datetime stamps for the data to get a value for how long a member has been a user.  It is possible that churn and life of an account may have some relevance\n",
    "- We could drill down further into the time analysis and look specifically around the timeframe that the account was canceled as it is possible that there is a difference in activity as we approach the churn event.\n",
    "- We could also leverage timestamp data to group our discreate users into users by time periods.  It may be that the activity in the first few weeks of the account creation have a direct impact on a future churn event\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
